Overview:
0. Convert to Pair RDD.
1. Basic usage of Key/Value pairs.
2. Advanced feature of using partitioning.

1. Creating Pair RDDs
Creating Pair RDDs is in fact easy. We just need to create a RDD composed of
tuples. Python and Scala have builtin tuple type, however, Java doesn't.
Instead, Java use the scala.Tuple2 class. This class provides ._1() and ._2()
functions to access its elements.

If we have a regular RDD. We can get a Pair RDD by using the map() function.
Remember from Chapter3, Java has an explicit specified RDD type, JavaPairRDD.
To create this special RDD, we need to use corresponding map function,
mapToPair(). One example is as follow:

PairFunction<String, String, String> keyData = 
    new PairFunction<String, String, String>() {
        public Tuple2<String, String> call(String x) {
            // use the first word as key
            return new Tuple2(x.split(" ")[0], x);
        }
    };
JavaPairRDD<String, String> pairs = lines.mapToPair(keyData);
    
2. Common Operations on Pair RDD
Pair RDD is still RDD. The special point is that, it composes of pairs(Tuple2
object in Java and Scala). It's not surprising Pair RDD supports common
operations defined for normal RDD.

For exmaple, to filter a Pair RDD, we can do as follow:
Function<Tuple2<String, String>, Boolean> longWordFilter = 
    new Function<Tuple2<String, String>, Boolean>() {
        public Boolean call(Tuple2<String, String> keyValue) {
            return (keyValue._2().length() < 20);
        }
    };
JavaPairRDD<String, String> result = pairs.filter(longWordFilter);

Basically, operations on Pair RDD doesn't have a big difference with
operations on normal RDD. I will skip the rest here.

3. Partition is used to group data in a certain way, in advance. Once the RDD
has been partitioned, further operations, which need partition, will not
partition this RDD any more. Instead, Spark will use the already existing
parition to complete operations such as join(), which is key-oriented. This
advanced partition can save both communication and operation. Thus, the
execution speed can improve dramastically.
(Refer to Figure 4-4 and 4-5 in book 'Learning Spark Lightning')

One caveat is, after partition a RDD, we have to use persist() to preserve the
partition. Otherwise, the partitioned information will be lost. Executing
further operations will re-evaluate and re-partition the RDD again.

Supplying partition information manually is not the only way to create a
partition. Many PairRDD operations creates a partition internally. These
operations include: cogroup(), groupWith(), join(), leftOuterJoin(),
rightOuterJoin(), ... , etc. So, after calling join() operation on two RDDs,
calling reduceByKey() on the joined result will be significantly faster, since
the reduceByKey() can use partition information from join().

Besides creating partition internally, some operations will also invalid
partition operations. An example is map().

