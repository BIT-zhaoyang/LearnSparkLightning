1. What's RDD?
RDD is simply an IMMUTABLE distributed collection of elements.

2. What's the role of RDD?
RDD is the core abstraction for working with data. In Spark, all work is
expressed as either creating new RDDS, transforming existing RDDs, or calling
operations on RDDs to compute a result.

3. RDD has two types of operations: transformations and actions. These two
types of operations are similar to the intermediate and terminal operations of
Java Stream class. RDD adopts lazy execution of for the operations. The
expression is not evaluated until an action operation is called on the RDD.

The theory of Spark is that, store as small data as possible. So, Sparks tries
to avoid storing any unnecessary data. Lazy execution is one way to realize
this goal. Also, all intermediate will be dropped. Each time an action is
called on a RDD, the whole process will re-run again.

If sometimes we do want to store the intermediate result for future reuse, we
can call 'persist()' on the RDD. This will store the RDD
in memory for future reuse.

4. Creating RDDs
There are two ways to create RDDs. One is to load data from external
resources, such as dataset. There are a lot of functions to do this. Another
way is using sc.parallelize() method to manually create a RDD. This way is
more on ad-hoc experiments, since the user has to manually specify all the
data and all the data will be stored in memory on a single machine.

5. More on RDD transformations
Transformations return a new RDD, while actions return some other type of
data. We already know that, transformations are not evaluated until an action
is called. As I have guessed, Spark build a DAG to store the relations between
each RDD, so that Spark knows how to compute the desired RDD later. Other than
computing the compute sequence, the DAG also helps to recover lost data in
persistent RDD.

6. Lazy evaluation also happens in loading data.

7. Passing Functions to Spark
Most of Spark's transformations, and some of its actions, depend on passing in
functions that are used by Spark to compute data. In Python and Scala, passing
functions as parameters seems easy, but in Java it's a bit more difficult.
Since functions must be defined as methods in some classes, we can only pass a
class which contains the desired functions, as parameter, to the target
function.

To fulfill this requirement, Spark has designed several functional interfaces
for Java. These interfaces are as follow:
    7.1 class Function<T, R> {
        public R call(T);   // Take in one input and return one output, for
                            // use with operations like map() and filter().
        }
     Example:
        JavaRDD<Integer> rdd = sc.parallelize(Arrays.asList(1, 2, 3, 4));
        JavaRDD<Integer> result = rdd.map(new Function<Integer, Integer>() {
            public Integer call(Integer x) { return x * x; }
        });

    7.2 class Function2<T1, T2, R> {
        public R call(T1, T2); // Take in two inputs and return one output,
                               // for use with operations like aggregate() or
                               // fold()
        }
     Example:
        Integer sum = rdd.reduce(new Function2<Integer, Integer, Integer>() {
            public Integer call(Integer x, Integer y) { return x + y; }
        });
        Integer multi = rdd.fold(0, new Function2<Integer, Integer, Integer>() {
            public Integer call(Integer x, Integer y) { return x * y; }
        });

    7.3 class FlatMapFunction<T, R> {
        public Iterable<R> call(T); // Take in one input and return zero or
                             // more outputs, for use with operations like
                             // flatMap().
        }
     Example:
        JavaRDD<String> lines = sc.parallelize(
                Arrays.asList("hello world","hi"));
        JavaRDD<String> words = lines.flatMap(
            new FlatMapFunction<String, String>() {
                public Iterable<String> call(String line) {
                    return Arrays.asList(line.split(" "));
                }
            });

8. Common Operations on RDD
8.1 Element-wise transformations
map(), flatMap(), filter()

8.2 Pseudo set operations
RDD1.distinct(), RDD1.union(RDD2),
RDD1.intersection(RDD2), RDD1.substract(RDD2)
The reason these operations are called pseudo set is that, RDD allows
duplicated elements. So, it's not strictly a set.

Among above operations, only union is a lightweight operation. All the other
operations return RDD contains only unique values. Removing duplicated
elements requires shuffling all the data over the network. Thus, operations
involving this process are expensive.

Additionally, RDD provides cartesian() operation, which is used for making
combinations. 

8.3 Actions
Aggregation actions: reduce(), fold(), and aggregate().

reduce() and fold() takes a function that operates on two elements of the type
in your RDD and returns a new element of the SAME type. A simple example can
be the summation computation, as shown in the 'Example' in 7.2. fold() is
almost the same with reduce(). The difference is that, it requires a initial
"identity value", which means it won't apply the value of an element, if it
was applied to that element multiple times.

aggregate() gives us more freedom. aggregate() also takes three elements. The
first element is the "identity element" of the returned type. This functions
as the initial accumulator. The second argument is a Function2 implementation
which defines the operation rules between the accumulator and the RDD
element(Because the types of the accumulator and the RDD element can be
different!). Then the third argument defines the operation rules between two
accumulators. One example is as follow, which computes the average value in a
RDD:
class AvgCount implements Serializable {    // The accumulator type
    public AvgCount(int total, int num) {
        this.total = total;
        this.num = num;
    }
    public int total;
    public int num;
    public double avg() {
        return total / (double) num;
    }
}
// Operation rules between the accumulator and RDD element
Function2<AvgCount, Integer, AvgCount> addAndCount = 
    new Function2<AvgCount, Integer, AvgCount>() {
        public AvgCount call(AvgCount a, Integer x) {
            a.total += x;
            a.num += 1;
            return a;
        }
    };
// Operation rules between accumulators
Function2<AvgCount, AvgCount, AvgCount> combine = 
    new Function2<AvgCount, AvgCount, AvgCount>() {
        public AvgCount call(AvgCount a, AvgCount b) {
            a.total += b.total;
            a.num += b.num;
            return a;
        }
    };
// Compute the average of RDD using aggregate()
AvgCount initial = new AvgCount(0, 0);  // declare init accumulator
AvgCount result = rdd.aggregate(initial, addAndCount, combine);
System.out.println(result.avg());

The other common actions return some or all of the data to our driver program.
Examples are: collect(), take(), and top().

9. Converting Between RDD Types
Certain types of RDD provides special functions. To access these certain
functions, we need to know how to convert to these special typed RDDs from
general ones.

The key is to use the map() function. However, to create the special RDDs,
special map() function must be used.

JavaRDD<Double> doubleRDD = integerRDD.map(x -> (double)x);
JavaDoubleRDD doubleRDD = integerRDD.mapToDouble(x -> (double)x);

The JavaDoubleRDD has special functions such as mean(), while JavaRDD<Double>
doesn't have.

Another special RDD type is JavaPairRDD.
